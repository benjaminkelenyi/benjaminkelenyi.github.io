---
title: "SAM-Net: Self-Attention based Feature Matching with Spatial Transformers and Knowledge Distillation"
category: manuscripts
collection: publications
permalink: /publication/2024-samnet
date: 2024-10-01
venue: 'Expert Systems with Applications'
paperurl: 'https://www.sciencedirect.com/science/article/pii/S0957417423033067?via%3Dihub'
citation: 'Benjamin Kelenyi, Victor Domsa, Levente Tamas,
SAM-Net: Self-Attention based Feature Matching with Spatial Transformers and Knowledge Distillation,
Expert Systems with Applications,
Volume 242,
2024,
122804,
ISSN 0957-4174,
https://doi.org/10.1016/j.eswa.2023.122804.
(https://www.sciencedirect.com/science/article/pii/S0957417423033067)
Abstract: In this research paper, we introduce a novel approach to enhance the performance of 2D feature matching and pose estimation through the integration of a hierarchical attention mechanism and knowledge distillation. Our proposed hierarchical attention mechanism operates at multiple scales, enabling both global context awareness and precise matching of 2D features, which is crucial for various computer vision tasks. To further improve our model’s performance, we incorporate insights from an existing model PixLoc (Sarlin et al., 2021) through knowledge distillation, effectively acquiring its behavior and capabilities by ignoring dynamic objects. SAM-Net outperforms state-of-the-art methods, validated on both indoor and outdoor public datasets. For the indoor dataset, our approach achieves remarkable AUC (5°/10°/20°) scores of 55.31/71.70/83.37. Similarly, for the outdoor dataset, we demonstrate outstanding AUC values of 26.01/46.44/63.61. Furthermore, SAM-Net achieves top ranking among published methods in two public visual localization benchmarks, highlighting the real benefits of the proposed method. The code and test suite can be accessed at link.11https://benjaminkelenyi.github.io/samnet/.
Keywords: Geometric features extraction; Self-attention; Knowledge-distillation; Spatial transformers; Pose estimation'
---
